[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, my name is Mfundo Monchwe and I’m a data scientist, reseacher, and freelancer. I have a passion for using data to solve complex problems and help orgnanizations make data-driven decision."
  },
  {
    "objectID": "about.html#introduction",
    "href": "about.html#introduction",
    "title": "About",
    "section": "",
    "text": "Hi, my name is Mfundo Monchwe and I’m a data scientist, reseacher, and freelancer. I have a passion for using data to solve complex problems and help orgnanizations make data-driven decision."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nI have a Bachelor’s degreee and Honors degree in Data Sciences from Sol Plaatje University. I have a background in Statistics, Mathematics, and Machine Learning.\n\nCurrently doing my Masters in Stastistics at the University of KwaZulu-Natal."
  },
  {
    "objectID": "about.html#work-experience",
    "href": "about.html#work-experience",
    "title": "About",
    "section": "Work Experience",
    "text": "Work Experience\n\nNotto Africa\nData Scientist–Consultant Johannesburg, SA Apr 2025 – Current\n\nDeveloped a data pipeline for the Notto Africa platform, which involved data extraction, transformation, and loading (ETL) processes.\nDeveloped a machine learning model to predict credit scores for users based on their financial history and behavior.\nDeveloped an API to integrate the machine learning model with the Notto Africa platform, allowing for real-time credit score predictions.\nCollaborated with the Notto Africa team to ensure the successful deployment and integration of the data pipeline and machine learning model into the platform.\nUsed KeyCloak for authentication and authorization of users on the Notto Africa platform.\nDeveloped Streamlit dashboards to visualize the data and provide insights into user behavior and credit scores.\n\n\n\nArena Holdings\nData Scientist\nJohannesburg, SA Oct 2024 – Jan 2025\n• Commercial reporting\n• Predictive modelling\n• Data analysis, visualization and engineering\n\n\nAvis Budget\nData Scientist Isando, HQ, Kempton Park, SA Apr 2024 – Current\n• Developed shiny dashboard for ilease product\n• Performed Analysis both on Data Quality and Documentation\n• Calculated IRR and Deviations for 6 countries\n• Developed credit risk recommendation system\n• Predicting future prices of a new vehicle using ML model\n\n\nAuditor General of South Africa\nTrainee Data Analyst\nPretoria East Head Office, SA\nMar 2024 – April 2024\n\nProvided data analytics support to various audit business units within the organization.\nDeveloped data analytics models.\nWorked closely with audit business units to identify the “Value-Add”.\nSupported the line manager in providing holistic, comprehensive data analytics reports.\n\n\n\nTutor\nBig Data Fundamentals and Introduction to Python Programming\n2021 - Taught content such as data wrangling, visualization, dealing with big data, and using Python to solve problems.\n\n\nFreelance Data Scientist and Researcher\n\nProvided guidance for academic master’s research and project execution, including data analysis, automating reports, data visualization, data wrangling, data cleaning, data mining, data engineering, machine learning, and statistics.\n\n\n\nProject Collaboration\nProtein Synthesis Analysis\nContract for 3 months\n\nAnalyzed protein synthesis in cells and performed statistical analysis on the data.\nWorked remotely on a project based in the USA using R and Python programming languages."
  },
  {
    "objectID": "about.html#skills",
    "href": "about.html#skills",
    "title": "About",
    "section": "Skills",
    "text": "Skills\n\nProgramming LanguagesData SkillsSoft SkillsTechniquesStatisticsTools\n\n\n\nPython\nR\nSQL\n\n\n\n\nData Wrangling\nData Analysis\nData Cleaning\nData Mining\nData Engineering\nData Science\n\n\n\n\nCommunication\nTeamwork\nLeadership\nTime Management\nProject Management\nCritical Thinking\nProblem Solving\n\n\n\n\nMachine Learning\nData Visualization\n\n\n\n\nStatistical methods\nBayesian Methods\nRegression Analysis\n\n\n\n\nDjango\nLinux\nGit\nGitHub\nMicrosoft Office\nTableau\nPower BI\nKeyCloak"
  },
  {
    "objectID": "about.html#contact-me",
    "href": "about.html#contact-me",
    "title": "About",
    "section": "Contact Me",
    "text": "Contact Me\nIf you would like to get in touch with me, feel free to send me an email at diditmfundo@gmail.com or connect with me on LinkedIn."
  },
  {
    "objectID": "Achievements.html",
    "href": "Achievements.html",
    "title": "Achievements",
    "section": "",
    "text": "1st prize on the mining digital problem at the 2022 University of Witwatersrand Hackathon\nPublished article 1 on Hackathon victory: Web link\nPublished article 2 on hackathon victory: Web link"
  },
  {
    "objectID": "Achievements.html#hackathon",
    "href": "Achievements.html#hackathon",
    "title": "Achievements",
    "section": "",
    "text": "1st prize on the mining digital problem at the 2022 University of Witwatersrand Hackathon\nPublished article 1 on Hackathon victory: Web link\nPublished article 2 on hackathon victory: Web link"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to my portfolio",
    "section": "",
    "text": "Data Scientist | Researcher | Freelancer"
  },
  {
    "objectID": "papers.html",
    "href": "papers.html",
    "title": "Papers",
    "section": "",
    "text": "Data security is a critical aspect of online transactions. It involves ensuring data confidentiality, data integrity, and data authenticity. While much has been done to improve data confidentiality, algorithms to address data integrity and data authenticity are rare. The RSA digital signature scheme is often used to address these issues, but it can be difficult to understand for non-experts. In this study, we showcase the implementation of a white-box RSA digital signature scheme, which is an algorithm used to ensure data confidentiality, integrity, and authenticity after online transactions. We build the proposed implementation based on the understanding that the RSA digital signature scheme is an asymmetric model that uses two keys. One key is used to sign data, and the other key is used to verify the signature. We evaluate the effectiveness of the white-box RSA digital signature scheme using a quantitative research approach, where we assess the execution time and signature verification accuracy for different values of p, q, and data lengths. We observe a tradeoff between security and execution time, and we recommend using large values of p and q for better data security.\n\n\nReference: Colin Chibaya, Mfundo Monchwe, Taryn Nicole Michael, Eli Bila Nimy. Showcasing White-Box Implementation of the RSA Digital Signature Scheme, American Journal of Computer Science and Technology. Volume 5, Issue 4, December 2022, pp. 198-203."
  },
  {
    "objectID": "papers.html#showcasing-white-box-implementation-of-the-rsa-digital-signature-scheme",
    "href": "papers.html#showcasing-white-box-implementation-of-the-rsa-digital-signature-scheme",
    "title": "Papers",
    "section": "",
    "text": "Data security is a critical aspect of online transactions. It involves ensuring data confidentiality, data integrity, and data authenticity. While much has been done to improve data confidentiality, algorithms to address data integrity and data authenticity are rare. The RSA digital signature scheme is often used to address these issues, but it can be difficult to understand for non-experts. In this study, we showcase the implementation of a white-box RSA digital signature scheme, which is an algorithm used to ensure data confidentiality, integrity, and authenticity after online transactions. We build the proposed implementation based on the understanding that the RSA digital signature scheme is an asymmetric model that uses two keys. One key is used to sign data, and the other key is used to verify the signature. We evaluate the effectiveness of the white-box RSA digital signature scheme using a quantitative research approach, where we assess the execution time and signature verification accuracy for different values of p, q, and data lengths. We observe a tradeoff between security and execution time, and we recommend using large values of p and q for better data security.\n\n\nReference: Colin Chibaya, Mfundo Monchwe, Taryn Nicole Michael, Eli Bila Nimy. Showcasing White-Box Implementation of the RSA Digital Signature Scheme, American Journal of Computer Science and Technology. Volume 5, Issue 4, December 2022, pp. 198-203."
  },
  {
    "objectID": "papers.html#bayesian-convolutional-neural-networks-for-coronavirus-lung-image-classification-with-uncertainty-estimation",
    "href": "papers.html#bayesian-convolutional-neural-networks-for-coronavirus-lung-image-classification-with-uncertainty-estimation",
    "title": "Papers",
    "section": "Bayesian Convolutional Neural Networks for Coronavirus Lung Image Classification with Uncertainty Estimation",
    "text": "Bayesian Convolutional Neural Networks for Coronavirus Lung Image Classification with Uncertainty Estimation\n\n\nPrevious attempts to identify or predict coronavirus using lung imaging data have not incorporated a way to quantify the uncertainty in their predictions. Additionally, these models need more certainty quantification to raise questions about their reliability. In this chapter, we address these issues by modeling a coronavirus classification model that utilizes a Bayesian convolutional neural networks (BCNNs) approach. This probabilistic machine learning approach allows for the estimation of uncertainty, providing insight into the reliability of coronavirus image classification. We test the accuracy of the model using a comprehensive radiographical lung image dataset, revealing its capability to deliver significant uncertainty information. Furthermore, we conduct comparisons with standard CNN models, highlighting the improved performance of the BCNN model in identifying complex cases that require further inspections.\n\n\nReference: Mfundo Monchwe, Ibidun C. Obagbuwa, Alfred Mwanza. Coronavirus Lung Image Classification With Uncertainty Estimation Using Bayesian Convolutional Neural Networks In: Hammouch, Z., Lahby, M., Baleanu, D. (eds) Mathematical Modeling and Intelligent Control for Combating Pandemics. Springer Optimization and Its Applications, vol 203. Springer, Cham. https://doi.org/10.1007/978-3-031-33183-1_8"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "This is simple shinydashboard of the previous project on A/B Testing for Website Themes. The goal is to determine which theme yields better user engagement, purchases, and conversion rates.\nSo basically just shows the results of the previous project in a shinydashboard."
  },
  {
    "objectID": "projects.html#digitizing-mining",
    "href": "projects.html#digitizing-mining",
    "title": "Projects",
    "section": "Digitizing Mining",
    "text": "Digitizing Mining\n\n\nDesigned and developed analytical web applications that enabled the digitization and analysis of complex mining documents.\nProgrammed functionalities that leveraged advanced techniques in descriptive, diagnostic, and predictive analytics to provide valuable insights to mining stakeholders.\nMining Document Digitizer Application link: Web link\nMining Data Analytics Web Application link: Web link\nSkills: R, Topic Modelling, Text Mining, K-means Clustering, Regression Analysis, OCR, Data Visualization"
  },
  {
    "objectID": "projects.html#bank-customer-churn",
    "href": "projects.html#bank-customer-churn",
    "title": "Projects",
    "section": "Bank Customer Churn",
    "text": "Bank Customer Churn\n\n\nDeveloped a Bank Customer prediction model using machine learning techniques such as xgboost to accurately identify customers at risk of leaving the bank.\nDeployed the model within a Streamlit application, allowing for user-friendly interaction and real-time predictions based on customer data.\nLeveraged the eli5 library for model interpretability, providing insightful explanations for predictions.\nStreamlit app link: Web link\nSkills: Python, Pandas, Joblib, eli5"
  },
  {
    "objectID": "projects.html#covid-19-classification",
    "href": "projects.html#covid-19-classification",
    "title": "Projects",
    "section": "COVID-19 Classification",
    "text": "COVID-19 Classification\n\n\nBuilt a Bayesian CNN model in Python to classify patients with COVID-19 using CT scan images with measurements of uncertainty.\nDemonstrated the potential of this approach to improve the accuracy and reliability of COVID-19 diagnosis in a research paper.\nSkills: Python, Deep Learning, TensorFlow, Image Processing, Computer Vision, Bayesian Inference"
  },
  {
    "objectID": "projects.html#mining-process-and-flotation-plant-database",
    "href": "projects.html#mining-process-and-flotation-plant-database",
    "title": "Projects",
    "section": "Mining Process and Flotation Plant Database",
    "text": "Mining Process and Flotation Plant Database\n\nThe main goal is to use this data to predict how much impurity is in the ore concentrate. As this impurity is measured every hour, if we can predict how much silica (impurity) is in the ore concentrate, we can help the engineers, giving them early information to take actions (empowering)!\nHence, they will be able to take corrective actions in advance (reduce impurity, if it is the case) and also help the environment (reducing the amount of ore that goes to tailings as you reduce silica in the ore concentrate).\n\n\n\n\n\n\nNote\n\n\n\nThe first column shows time and date range (from march of 2017 until september of 2017). Some columns were sampled every 20 second. Others were sampled on a hourly base.\nThe second and third columns are quality measures of the iron ore pulp right before it is fed into the flotation plant. Column 4 until column 8 are the most important variables that impact in the ore quality in the end of the process. From column 9 until column 22, we can see process data level and air flow inside the flotation columns, which also impact in ore quality. The last two columns are the final iron ore pulp quality measurement from the lab. Target is to predict the last column, which is the % of silica in the iron ore concentrate."
  },
  {
    "objectID": "projects.html#weather-forecast",
    "href": "projects.html#weather-forecast",
    "title": "Projects",
    "section": "Weather Forecast",
    "text": "Weather Forecast\n\nDescription:\nWeather forecasting is the task of forecasting weather conditions for a given location and time. With the use of weather data and algorithms, it is possible to predict weather conditions for the next n number of days.\nFor forecasting weather using Python, we need a dataset containing historical weather data based on a particular location.\nTo Download the dataset, click on the link."
  },
  {
    "objectID": "projects.html#api-spotify-requests",
    "href": "projects.html#api-spotify-requests",
    "title": "Projects",
    "section": "API Spotify Requests",
    "text": "API Spotify Requests\n\n\nThis project is a simple API request to Spotify API. It is a simple project to show how to use API requests in Python.\nThe goal of this project is to get the top 10 songs of an artist and create a playlist with them.\nCreated a scripts that generates music recommmendations based on a seed track using the Spotify API. Which demonstrated how to authenticate with the API using client credentials and how to make request for music recommendations.\nCreated a script that demonstrates how to retrieve information about a track, artist, album and playlist using the Spotify API. Which demonstrated how to authenticate with the API using client credentials and how to make request for artist recommendations.\nCreated a Spotify Playlist Data Collection script, which in return shows an example of how to authenticate the API using client credentials and how to make request for playlist data.\nFor more information on how to collect data from a Spotify playlist using the Spotify API, please refer to the Spotify Web API documentation"
  },
  {
    "objectID": "projects.html#ab-testing-for-website-themes",
    "href": "projects.html#ab-testing-for-website-themes",
    "title": "Projects",
    "section": "A/B Testing for Website Themes",
    "text": "A/B Testing for Website Themes\n\nThis project analyzes the performance of two different themes for a website using A/B testing. The goal is to determine which theme yields better user engagement, purchases, and conversion rates.\n\nDataset\nThe dataset contains the following metrics for each theme:\n\nClick Through Rate\nConversion Rate\nBounce Rate\n\n\n\nHypothesis Testing\nWe performed a t-test for each metric to compare the means of the two groups (Light Theme and Dark Theme). We calculated the mean, standard deviation, t-statistic, p-value, and effect size for each metric.\nBased on the results of the hypothesis tests, we cannot conclude that one theme yields better user engagement, purchases, and conversion rates than the other. For Click Through Rate and Bounce Rate, the Dark Theme had a higher mean than the Light Theme, but the difference was not statistically significant (p-value &gt; 0.05). For Conversion Rate, there was no statistically significant difference between the two themes (p-value &gt; 0.05).\n\n\nConclusion\nWe cannot make a definitive conclusion about which theme is better based on these results. However, you can use these results to inform your decision about which theme to use for your website."
  },
  {
    "objectID": "projects.html#e-commerce-rfm-analysis",
    "href": "projects.html#e-commerce-rfm-analysis",
    "title": "Projects",
    "section": "E-Commerce RFM Analysis",
    "text": "E-Commerce RFM Analysis\n\nRFM analysis is a powerful technique used by companies to better understand customer behaviour and optimize engagement strategies. It revolves around three key dimensions: recency, frequency, and monetary value. These dimensions capture essential aspects of customer transactions, providing valuable information for segmentation and personalized marketing campaigns.\nThe given dataset is provided by an e-commerce platform containing customer transaction data including customer ID, purchase date, transaction amount, product information, ID command and location. The platform aims to leverage RFM (recency, frequency, monetary value) analysis to segment customers and optimize customer engagement strategies.\n\nRFM analysis is a powerful technique used by companies to better understand customer behaviour and optimize engagement strategies. It revolves around three key dimensions: recency, frequency, and monetary value.\nThese dimensions capture essential aspects of customer transactions, providing valuable information for segmentation and personalized marketing campaigns.\nThe given dataset is provided by an e-commerce platform containing customer transaction data including customer ID, purchase date, transaction amount, product information, ID command and location.\nThe platform aims to leverage RFM (recency, frequency, monetary value) analysis to segment customers and optimize customer engagement strategies.\n\n\nResults\nThe analysis provides insights into customer behavior and identification of high-value customers, at-risk customers, and potential opportunities for personalized marketing campaigns. The following tasks were performed:\n\nRFM analysis was performed on the given dataset to segment customers into different groups based on their RFM scores.\nA detailed analysis of the customer segments was provided, including the distribution of RFM scores, the characteristics of each segment, and actionable insights for the platform to optimize customer engagement strategies.\nThe following table shows the RFM customer segments and their corresponding RFM score ranges:\n\n\n\nRFM Customer Segment\nRFM Score Range\n\n\n\n\nChampions\n9-15\n\n\nPotential\n6-8\n\n\nAt-Risk Customers\n5\n\n\nCan’t Lose Them\n4-5\n\n\nLost Customers\n3\n\n\n\n\n\n\nConclusion\nThe RFM analysis provides valuable insights into customer behavior and segmentation, enabling the platform to optimize customer engagement strategies and improve customer satisfaction. The platform can use the insights gained from the analysis to identify high-value customers, at-risk customers, and potential opportunities for personalized marketing campaigns.\n\\[\nargmin_{C_1, \\ldots, C_K} \\sum_{k=1}^{K} \\sum_{x \\in S_k} | x - C_k |^2\n\\]\nIn summary, clustering is a powerful technique for grouping similar data points together, and K-means clustering is a popular algorithm for many applications, including customer segmentation. By using clustering to identify groups of customers with similar behavior, we can develop targeted marketing strategies to improve customer retention and increase sales."
  },
  {
    "objectID": "Achievements.html#microsoft-training-trophies-and-achievements",
    "href": "Achievements.html#microsoft-training-trophies-and-achievements",
    "title": "Achievements",
    "section": "Microsoft Training Trophies and Achievements",
    "text": "Microsoft Training Trophies and Achievements\n\nCreate machine learning models\nGet Started With Microsoft data analytics\nAZ-400 Get Started on a DevOps transformation journey\nIntroduction to Machine Learning Operations (MLOps)\nExplore Azure Machine Learning workspace resources and assets\nTrain and evaluate deep learnnig models\nPlan Agile with Github Projects and Azure Boards\nGet started building with Power BI\nContinuous deployment for machine learning\nGet Started with AI on Azure"
  },
  {
    "objectID": "projects.html#credit-scoring-and-segmentation",
    "href": "projects.html#credit-scoring-and-segmentation",
    "title": "Projects",
    "section": "Credit Scoring and Segmentation",
    "text": "Credit Scoring and Segmentation\n\n\nCredit scoring is a statistical analysis performed by lenders and financial institutions to access a person’s creditworthiness. Lenders use credit scoring, among other things, to decide on whether to extend or deny credit.\nCredit Segmentation refers to the process of dividing the customers into groups based on their credit behavior. The customers are divided into different segments based on their credit score, credit history, and other factors.\nThe goal of this project is to segment customers into different groups based on their credit behavior.\nYou can access the dataset via this link here https://statso.io/credit-scoring-case-study/ or you can download it from the data folder in this repository.\n\n\nOverview\nCredit scoring aims to determine the creditworthiness of individuals based on their credit profiles. By analyzing factors such as payment history, credit utilization ratio, and number of credit accounts, we can assign a credit score to each individual, providing a quantitative measure of their creditworthiness.\nThe given dataset includes features such as age, gender, marital status, education level, employment status, credit utilization ratio, payment history, number of credit accounts, loan amount, interest rate, loan term, type of loan, and income level."
  },
  {
    "objectID": "projects.html#shinyapp-on-ab-testing-theme",
    "href": "projects.html#shinyapp-on-ab-testing-theme",
    "title": "Projects",
    "section": "",
    "text": "This is simple shinydashboard of the previous project on A/B Testing for Website Themes. The goal is to determine which theme yields better user engagement, purchases, and conversion rates.\nSo basically just shows the results of the previous project in a shinydashboard."
  },
  {
    "objectID": "about.html#skills-1",
    "href": "about.html#skills-1",
    "title": "About",
    "section": "Skills",
    "text": "Skills\n\n\n\nTechnical Skills\nData Skills\nSoft Skills\n\n\n\n\nAWS (Redshift)\nData Wrangling\nCommunication\n\n\nPython\nData Analysis\nTeamwork\n\n\nR\nData Cleaning\nLeadership\n\n\nSQL\nData Mining\nTime Management\n\n\nMachine Learning\nData Engineering\nProject Management\n\n\nData Visualization\nData Science\nCritical Thinking\n\n\nStatistics\n\nProblem Solving\n\n\nMathematics\n\n\n\n\nResearch\n\n\n\n\nBayesian Methods\n\n\n\n\nRegression Analysis\n\n\n\n\nDjango\n\n\n\n\nLinux\n\n\n\n\nGit\n\n\n\n\nGitHub\n\n\n\n\nMicrosoft Office\n\n\n\n\nGoogle Suite"
  },
  {
    "objectID": "about.html#programming-languages",
    "href": "about.html#programming-languages",
    "title": "About",
    "section": "Programming Languages",
    "text": "Programming Languages\n\nPython\nR\nSQL"
  },
  {
    "objectID": "about.html#data-skills",
    "href": "about.html#data-skills",
    "title": "About",
    "section": "Data Skills",
    "text": "Data Skills\n\nData Wrangling\nData Analysis\nData Cleaning\nData Mining\nData Engineering\nData Science"
  },
  {
    "objectID": "about.html#soft-skills",
    "href": "about.html#soft-skills",
    "title": "About",
    "section": "Soft Skills",
    "text": "Soft Skills\n\nCommunication\nTeamwork\nLeadership\nTime Management\nProject Management\nCritical Thinking\nProblem Solving"
  },
  {
    "objectID": "about.html#techniques",
    "href": "about.html#techniques",
    "title": "About",
    "section": "Techniques",
    "text": "Techniques\n\nMachine Learning\nData Visualization"
  },
  {
    "objectID": "about.html#statistics",
    "href": "about.html#statistics",
    "title": "About",
    "section": "Statistics",
    "text": "Statistics\n\nStatistics\nBayesian Methods\nRegression Analysis"
  },
  {
    "objectID": "about.html#tools",
    "href": "about.html#tools",
    "title": "About",
    "section": "Tools",
    "text": "Tools\n\nDjango\nLinux\nGit\nGitHub\nMicrosoft Office"
  },
  {
    "objectID": "about.html#auditor-general-of-south-africa",
    "href": "about.html#auditor-general-of-south-africa",
    "title": "About",
    "section": "Auditor General of South Africa",
    "text": "Auditor General of South Africa\nPretoria East Head Office, SA\nMar 2024 – April 2024 - Provided data analytics support to various audit business units within the organization. - Developed data analytics models. - Worked closely with audit business units to identify the “Value-Add”. - Supported the line manager in providing holistic, comprehensive data analytics reports."
  }
]